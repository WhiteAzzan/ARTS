## Algorithm

**242.有效字母的异位词**

给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的一个字母异位词。

示例 1:

输入: s = "anagram", t = "nagaram"
输出: true
示例 2:

输入: s = "rat", t = "car"
输出: false

思路：

第一种方法：排序

因为字母都是一样的才行，那么我们直接让string字符串按照顺序排列下来，一样的即可。很简单。但是这里时间复杂度最低也是NlogN，就算是快排。

第二种方法：Map：计数

{letter： Count}

Map -> Count {a:3, n:1.....}

时间复杂度来看，这只是在计数，所以是O(N)，如果再加上插入删除和查询，都是O(1)，所以合起来也是O(N)，所以这种方法略快于排序的算法。

代码：

```
class Solution:
    def isAnagram(self, s, t):
        """
        :type s: str
        :type t: str
        :rtype: bool
        """
#         dic1, dic2 = {}, {}
#         for item in s:
#             dic1[item] = dic1.get(item, 0) + 1
#         for item in t:
#             dic2[item] = dic2.get(item, 0) + 1
            
#         return dic1 == dic2

        dic1, dic2 = [0]*26, [0]*26 #一共也就26个英文字母
        for item in s:
            dic1[ord(item)-ord('a')] += 1 #计数中~
            
        for item in t:
            dic2[ord(item)-ord('a')] += 1
        
        return dic1 == dic2
```


## Review

VGG论文

**小卷积核**

说到网络深度，这里就不得不提到卷积，虽然AlexNet有使用了11x11和5x5的大卷积，但大多数还是3x3卷积，对于stride=4的11x11的大卷积核，我认为在于一开始原图
的尺寸很大因而冗余，最为原始的纹理细节的特征变化用大卷积核尽早捕捉到，后面的更深的层数害怕会丢失掉较大局部范围内的特征相关性，后面转而使用更多3x3的小
卷积核（和一个5x5卷积）去捕捉细节变化。

而VGGNet则清一色使用3x3卷积。因为卷积不仅涉及到计算量，还影响到感受野。前者关系到是否方便部署到移动端、是否能满足实时处理、是否易于训练等，后者关系到
参数更新、特征图的大小、特征是否提取的足够多、模型的复杂度和参数量等等。

总结一下，我们可以得出两个结论：

同样stride下，不同卷积核大小的特征图和卷积参数差别不大；

越大的卷积核计算量越大。

其实对比参数量，卷积核参数的量级在十万，一般都不会超过百万。相比全连接的参数规模是上一层的feature map和全连接的神经元个数相乘，这个计算量也就更大了。
其实一个关键的点——多个小卷积核的堆叠比单一大卷积核带来了精度提升，这也是最重要的一点。

**感受野**

说完了计算量我们再来说感受野。这里给出一张VGG作者的PPT，作者在VGGNet的实验中只用了两种卷积核大小：1x1和3x3。作者认为两个3x3的卷积堆叠获得的感受野大
小，相当一个5x5的卷积；而3个3x3卷积的堆叠获取到的感受野相当于一个7x7的卷积。
![](https://img-blog.csdn.net/20180109173248800)

既然说到了VGG清一色用小卷积核，结合作者和自己的观点，这里整理出小卷积核比用大卷积核的三点优势：

更多的激活函数、更丰富的特征，更强的辨别能力。卷积后都伴有激活函数，更多的卷积核的使用可使决策函数更加具有辨别能力，此外就卷积本身的作用而言，3x3比7x7
就足以捕获特征的变化：3x3的9个格子，最中间的格子是一个感受野中心，可以捕获上下左右以及斜对角的特征变化。主要在于3个堆叠起来后，三个3x3近似一个7x7，
网络深了两层且多出了两个非线性ReLU函数，（特征多样性和参数参数量的增大）使得网络容量更大（关于model capacity，AlexNet的作者认为可以用模型的深度和
宽度来控制capacity），对于不同类别的区分能力更强（此外，从模型压缩角度也是要摒弃7x7，用更少的参数获得更深更宽的网络，也一定程度代表着模型容量，后人
也认为更深更宽比矮胖的网络好）；

卷积层的参数减少。相比5x5、7x7和11x11的大卷积核，3x3明显地减少了参数量，这点可以回过头去看上面的表格。比方input channel数和output channel数均为C，
那么3层conv3x3卷积所需要的卷积层参数是：3x(Cx3x3xC)=27C^2，而一层conv7x7卷积所需要的卷积层参数是：Cx7x7xC=49C^2。conv7x7的卷积核参数比conv3x3多
了(49-27)/27x100% ≈ 81%；

小卷积核代替大卷积核的正则作用带来性能提升。作者用三个conv3x3代替一个conv7x7，认为可以进一步分解（decomposition）原本用7x7大卷积核提到的特征，这里
的分解是相对于同样大小的感受野来说的。关于正则的理解我觉得还需要进一步分析。

其实最重要的还是多个小卷积堆叠在分类精度上比单个大卷积要好。


