### Algorithm



### Review

下次一定写！！！！

### Tips

神经网络中比较重要的有激活函数。有很多类型的，但是各有优缺点。

**sigmoid**是最早的，非线性函数，但是有三个缺点：

1.饱和神经元会杀死梯度，因为饱和的时候，即负的很大，或者正的很多大的时候，就会导致梯度为0，gradient vanish

2.输出不是零中心

3.exp（）指数函数计算的代价比较高
所以我们一般不用

**tanh（x）**
这个是以零为中心，但是仍然会出现梯度消失的情况

**RelU**
他不会饱和，而且max函数计算的效率很高。但是在负半轴的时候，是0啊！

所以有新的激活函数出现：Leaky ReLU

他既不会出现梯度消失，而且计算高效。因为他的负半轴是f(X)=max(αx，x)

**ELU**

是ReLU版本中最好的一种，更接近零均值输出。而且负区域的饱和情况相对于Leaky ReLU更平滑。

### Share

阿姆达尔定律

这是硬件行业的准则，而且现在也广泛应用在软件行业

计算公式：

S = 1/[(1-P) + P/s]

大S代表着系统最后的性能提升，右边分母中的小写的s，代表某一项指标的性能提升，比如把计算机中的内存的速度提升了两倍，右边的小s就是两倍。p代表这项提升被用到的比例(或者说概率，因为在计算机中一个局部对计算时间的影响是估计出来的，因此用概率p代表），比如说内存的读写访问，占了计算机程序运行的20%的时间。

对于个人而言，阿姆达尔定律也应该是我们做事情的准则。

那些只能产生1%效果的事情，你就是把结果提高100倍，影响力也很有限；相反，那些占到了一般以上效果的事情，哪怕改进5%，至少我们能看到2.5%的提高。

当然，当一些问题得到了解决以后，他们的重要性就下降了，这时我们需要寻找新的重要问题去解决。整个IT行业就是这样自觉和不自觉的，滚动的进步的，而每一个人都可以用这个法则审视一下自己该做和暂时不该做的事情。
