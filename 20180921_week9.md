## ARTS

### Algorithm
这次再来回顾一下链表的算法实现

1.反转链表
反转一个单链表。

示例:

输入: 1->2->3->4->5->NULL
输出: 5->4->3->2->1->NULL
进阶:
你可以迭代或递归地反转链表。你能否用两种方法解决这道题？

思路：一般想到的就是将链表的后续结点的指针指向前继结点，但是如何去实现呢？
```
class Solution:
    def reverseList(self, head):
        """
        :type head: ListNode
        :rtype: ListNode
        """
        
        cur,prev = head,None
        while cur:
            cur.next,prev,cur = prev,cur,cur.next  ###这样一行代码即可表示三个变量之间的赋值
        return prev
```

2.链表交换相邻元素-swap node
思路：类似的，只要把相邻的两个结点互相指向即可。
```
def swapPairs(self, head)
  pre,pre.next = self,head
  while pre.next and pre.next.next:
    a = pre.next
    b = a.next
    pre.next, b.next, a.next = b,a,b.next  
    pre = a
  return self.next
```



3.环形链表
给定一个链表，判断链表中是否有环。

进阶：
你能否不使用额外空间解决此题？

思路：这里有一个快慢指针的技巧，就是设置两个指针，一个走两步，另一个走一步，一开始都指向头结点，之后如果相遇则说明是环形，如果遇不到(不等于）则说明不是环形。

```
class Solution(object):
    def hasCycle(self, head):
        """
        :type head: ListNode
        :rtype: bool
        """
        fast = slow = head
        while slow and fast and fast.next:
            slow = slow.next
            fast = fast.next.next
            if slow is fast:
                return True
        return False
```

### Review

![cs231n线性分类笔记和最优化](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture03.pdf)

之前讲了knn的简单分类器，之所以简单，是因为它不需要去学习参数，只需要调整k值去改变分类的程度，而且knn严格根据距离来判断，所以出现偏差不一致的图片就不能很好区分。所以就有了线性分类器：SVM和softmax分类器。

SVM分类器是根据一个loss function来判断是否超过了delta的范围。最后需要加上正则化项去防止过拟合；softmax分类器利用softmax函数去分类，通过求得概率的分布情况来得到结果。

最优化其实是机器学习中的一个关键问题，梯度下降是对神经网络的损失函数最优化中最常用的方法。


### Tips

#### 房价预测的步骤

训练集的制作
```
train1 = train1.drop(['Id','SalePrice'],axis=1)
X = pd.get_dummies(train1).reset_index(drop = True)  #one-hot编码
from sklearn.metrics import mean_squared_error
matrix = xgb.DMatrix(data = X,label=y)

#数据集进行切分，80%训练，20%用来测试
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = trian_test_split(X,y,test_size=0.2,random_state=123)

#XGboost
xg_reg = xgb.XGBRegressor(objective = 'reg:Linear',colsample_bytree=0.4,learning_rate=0.01,max_depth=8,alpha=10,n_estimator=600,subsample=0.7)
xg_reg.fit(X_train,y_train)
pred = xg_reg.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test,pred))
logrmse = np.sqrt(mean_squared_error(np.log(y_test),np.log(pred))
```
Xgboost中的超参：
learning_rate 学习率(0,1)
max_depth:Boosting过程中每一个树的最大深度
colsample：列采样率
subsample：每棵树的样本采样率
n_estimator：子树的数量
object：树的学习模型

评测过程两步骤

1：predict(X_test)得到评测的结果

2：与真正的测试集y_test进行比较，比较函数使用mse或logmse

然后最后通过不断地调参得到较好的结果分数

### Share

分享一段撒切尔夫人的话：

注意你的态度，因为它能影响你的想法

注意你的想法，因为它能决定你的言辞和行动

注意你的言辞和行动，因为他能主导你的行为

注意你的行为，因为它能变成你的习惯

注意你的习惯，因为它能塑造你的性格

注意你的性格，因为它能决定你命运

我们每做一件事情都是经过   暗示->行为->习惯->奖励  这样一个循环来的，坏习惯的养成恰好是一个特定的环境自己受到某种暗示，然后就做了某种行为，这样的行为就会造就你的习惯，这个习惯就很难改变，你得到了暂时的快感和满足，但是这又是空虚无聊痛苦的，长久之计是做那些暂时痛苦的事情，长久来看将会是无穷的收获
