## ARTS

### Algorithm


### Review

![cs231n线性分类笔记和最优化](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture03.pdf)

之前讲了knn的简单分类器，之所以简单，是因为它不需要去学习参数，只需要调整k值去改变分类的程度，而且knn严格根据距离来判断，所以出现偏差不一致的图片就不能很好区分。所以就有了线性分类器：SVM和softmax分类器。

SVM分类器是根据一个loss function来判断是否超过了delta的范围。最后需要加上正则化项去防止过拟合；softmax分类器利用softmax函数去分类，通过求得概率的分布情况来得到结果。

最优化其实是机器学习中的一个关键问题，梯度下降是对神经网络的损失函数最优化中最常用的方法。


### Tips

#### 房价预测的步骤

训练集的制作
```
train1 = train1.drop(['Id','SalePrice'],axis=1)
X = pd.get_dummies(train1).reset_index(drop = True)  #one-hot编码
from sklearn.metrics import mean_squared_error
matrix = xgb.DMatrix(data = X,label=y)

#数据集进行切分，80%训练，20%用来测试
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = trian_test_split(X,y,test_size=0.2,random_state=123)

#XGboost
xg_reg = xgb.XGBRegressor(objective = 'reg:Linear',colsample_bytree=0.4,learning_rate=0.01,max_depth=8,alpha=10,n_estimator=600,subsample=0.7)
xg_reg.fit(X_train,y_train)
pred = xg_reg.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test,pred))
logrmse = np.sqrt(mean_squared_error(np.log(y_test),np.log(pred))
```
Xgboost中的超参：
learning_rate 学习率(0,1)
max_depth:Boosting过程中每一个树的最大深度
colsample：列采样率
subsample：每棵树的样本采样率
n_estimator：子树的数量
object：树的学习模型

评测过程两步骤

1：predict(X_test)得到评测的结果

2：与真正的测试集y_test进行比较，比较函数使用mse或logmse

然后最后通过不断地调参得到较好的结果分数

### Share

分享一段撒切尔夫人的话：

注意你的态度，因为它能影响你的想法

注意你的想法，因为它能决定你的言辞和行动

注意你的言辞和行动，因为他能主导你的行为

注意你的行为，因为它能变成你的习惯

注意你的习惯，因为它能塑造你的性格

注意你的性格，因为它能决定你命运

我们每做一件事情都是经过   暗示->行为->习惯->奖励  这样一个循环来的，坏习惯的养成恰好是一个特定的环境自己受到某种暗示，然后就做了某种行为，这样的行为就会造就你的习惯，这个习惯就很难改变，你得到了暂时的快感和满足，但是这又是空虚无聊痛苦的，长久之计是做那些暂时痛苦的事情，长久来看将会是无穷的收获
